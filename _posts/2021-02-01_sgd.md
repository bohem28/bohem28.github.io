gradient descent 파헤치기

출처 : https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1

에 multi dimensional 덧붙임

# Gradient Descent

Cost funcion $J(\theta)$ of Linear Regression

$ h_{\theta} (x^i) = \theta_0 + \theta_1*x$

$ J(\theta) = 1/2m \sum_{i=1}^{m}(\hat{y}^i -y^i)^2 = 1/2m \sum_{i=1}^{m}(h_{\theta}(x^i) -y^i)^2  $





## Batch Gradient Descent

obs가 m개 있는 상황에서, m개를 다 써서 기울기 update하겠다는 뜻

$\theta_j \equiv \theta_{j} - \alpha * \frac{\partial}{\partial\theta_j} 1/2m \sum_{i=1}^{m}(h_{\theta}(x^i) -y^i)^2  $

$ = \theta_{j} - \alpha * \frac{\partial}{\partial\theta_j} 1/m \sum_{i=1}^{m}(\hat{y}^i -y^i)x_j^i $

Multi dimensional)

$\theta \equiv \theta - \alpha*X^T(X\theta-Y)$

$X$ n*p

$\theta$ p*1

$Y$ n*1







## Stochastic Gradient Descent

obs가 m개 있는 상황에서, 하나씩 사용해서 기울기 update하겠다



for k in range(m):

​	$\theta_j \equiv \theta_{j} - \alpha * \frac{\partial}{\partial\theta_j}  1/2(h_{\theta}(x^k) -y^k)^2  $

   $ = \theta_{j} - \alpha * (h_{\theta}(x^k) -y^k)x^k_j $



Multi dimensional)

$  \frac{\partial}{\partial\theta}  1/2(\underline{x}\theta - y)(\underline{x}\theta - y) = \frac{\partial}{\partial\theta}1/2 ( \underline{x}\theta\underline{x}\theta -2\underline{x}\theta y + y^2 ) $
$ = 1/2 (2\underline{x}^T\underline{x}\theta-2\underline{x}^Ty) = (\underline{x}^T\underline{x}\theta-\underline{x}^Ty) = \underline{x}^T (\underline{x}\theta-y) $

Where $\underline{x}$ 1*p, $\underline{x}\theta$ is constant


## Mini Batch Stochastic Gradient Descent

obs m개 중에서 s개 사용해서 기울기 update하겠다

for batch in range(bathlist):

   $\theta_j \equiv \theta_{j} - \alpha * \frac{\partial}{\partial\theta_j} J_{batch}(\theta)$
   $\theta_j \equiv \theta_{j} - \alpha * \frac{\partial}{\partial\theta_j} 1/2s \sum_{i=1}^{s}(h_{\theta}(x^i) -y^i)^2  $ 	



Multidimensional	

$\theta \equiv \theta - \alpha*X'^T(X'\theta-Y')$

Reduced form of $X,\theta,Y$

$X'$ s*p

$\theta$ p*1

$Y'$ s*1

